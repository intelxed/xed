#BEGIN_LEGAL
#
#Copyright (c) 2019 Intel Corporation
#
#  Licensed under the Apache License, Version 2.0 (the "License");
#  you may not use this file except in compliance with the License.
#  You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
#  Unless required by applicable law or agreed to in writing, software
#  distributed under the License is distributed on an "AS IS" BASIS,
#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#  See the License for the specific language governing permissions and
#  limitations under the License.
#  
#END_LEGAL
AVX_INSTRUCTIONS()::

# Issues: encoder is at a loss for vmaddps xmm0,xmm0,xmm0,xmm0.
# Encoder must enforce equality between two parameters. Never had to do this before.
#   Extra check?
# Decoder must rip off suffixes _DDMR, _DDRM, _DRMD  in disassembly (eventually)
#############################################################################################
# Operand orders:
#             A  =  B   *  C     +  D
#Type 1)   reg0  reg0  mem/reg1  reg2          DDMR  312 or 132
#Type 2)   reg0  reg0  reg1      mem/reg2      DDRM  123 or 213
#Type 3)   reg0  reg1  mem/reg2  reg0          DRMD  321 or 231

# dst is in MODRM.REG
# regsrc is in VEX.vvvv
# memop is in MODRM.RM
############################################################################################












##########################################################












##################################################################













##################################################################
{
ICLASS    : VFMADD132PD
EXCEPTIONS: avx-type-2
CPL       : 3
CATEGORY  : VFMA
EXTENSION : FMA
ATTRIBUTES: MXCSR
# R/M 128
PATTERN : VV1 0x98 VL128 V66 V0F38 W1 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()
OPERANDS  : REG0=XMM_R():rw:dq:f64 REG1=XMM_N():r:dq:f64 MEM0:r:dq:f64
# R/R 128
PATTERN : VV1 0x98 VL128 V66 V0F38 W1 MOD[0b11] MOD=3 REG[rrr] RM[nnn]
OPERANDS  : REG0=XMM_R():rw:dq:f64 REG1=XMM_N():r:dq:f64 REG2=XMM_B():r:dq:f64


# R/M 256
PATTERN : VV1 0x98 VL256 V66 V0F38 W1 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()
OPERANDS  : REG0=YMM_R():rw:qq:f64 REG1=YMM_N():r:qq:f64 MEM0:r:qq:f64
# R/R 256
PATTERN : VV1 0x98 VL256 V66 V0F38 W1 MOD[0b11] MOD=3 REG[rrr] RM[nnn]
OPERANDS  : REG0=YMM_R():rw:qq:f64 REG1=YMM_N():r:qq:f64 REG2=YMM_B():r:qq:f64
}
{
ICLASS    : VFMADD132PS
EXCEPTIONS: avx-type-2
CPL       : 3
CATEGORY  : VFMA
EXTENSION : FMA
ATTRIBUTES: MXCSR
# R/M 128
PATTERN : VV1 0x98 VL128 V66 V0F38 W0 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()
OPERANDS  : REG0=XMM_R():rw:dq:f32 REG1=XMM_N():r:dq:f32 MEM0:r:dq:f32
# R/R 128
PATTERN : VV1 0x98 VL128 V66 V0F38 W0 MOD[0b11] MOD=3 REG[rrr] RM[nnn]
OPERANDS  : REG0=XMM_R():rw:dq:f32 REG1=XMM_N():r:dq:f32 REG2=XMM_B():r:dq:f32


# R/M 256
PATTERN : VV1 0x98 VL256 V66 V0F38 W0 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()
OPERANDS  : REG0=YMM_R():rw:qq:f32 REG1=YMM_N():r:qq:f32 MEM0:r:qq:f32
# R/R 256
PATTERN : VV1 0x98 VL256 V66 V0F38 W0 MOD[0b11] MOD=3 REG[rrr] RM[nnn]
OPERANDS  : REG0=YMM_R():rw:qq:f32 REG1=YMM_N():r:qq:f32 REG2=YMM_B():r:qq:f32
}
{
ICLASS    : VFMADD132SD
EXCEPTIONS: avx-type-3
CPL       : 3
CATEGORY  : VFMA
EXTENSION : FMA
ATTRIBUTES: MXCSR simd_scalar
# R/M 128
PATTERN : VV1 0x99 V66 V0F38 W1 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()
OPERANDS  : REG0=XMM_R():rw:dq:f64 REG1=XMM_N():r:q:f64 MEM0:r:q:f64
# R/R 128
PATTERN : VV1 0x99 V66 V0F38 W1 MOD[0b11] MOD=3 REG[rrr] RM[nnn]
OPERANDS  : REG0=XMM_R():rw:dq:f64 REG1=XMM_N():r:q:f64 REG2=XMM_B():r:q:f64
}
{
ICLASS    : VFMADD132SS
EXCEPTIONS: avx-type-3
CPL       : 3
CATEGORY  : VFMA
EXTENSION : FMA
ATTRIBUTES: MXCSR simd_scalar
# R/M 128
PATTERN : VV1 0x99  V66 V0F38 W0 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()
OPERANDS  : REG0=XMM_R():rw:dq:f32 REG1=XMM_N():r:d:f32 MEM0:r:d:f32
# R/R 128
PATTERN : VV1 0x99  V66 V0F38 W0 MOD[0b11] MOD=3 REG[rrr] RM[nnn]
OPERANDS  : REG0=XMM_R():rw:dq:f32 REG1=XMM_N():r:d:f32 REG2=XMM_B():r:d:f32

}

{
ICLASS    : VFMADD213PD
EXCEPTIONS: avx-type-2
CPL       : 3
CATEGORY  : VFMA
EXTENSION : FMA
ATTRIBUTES: MXCSR
# R/M 128
PATTERN : VV1 0xA8 VL128 V66 V0F38 W1 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()
OPERANDS  : REG0=XMM_R():rw:dq:f64 REG1=XMM_N():r:dq:f64    MEM0:r:dq:f64
# R/R 128
PATTERN : VV1 0xA8 VL128 V66 V0F38 W1 MOD[0b11] MOD=3 REG[rrr] RM[nnn]
OPERANDS  : REG0=XMM_R():rw:dq:f64 REG1=XMM_N():r:dq:f64 REG2=XMM_B():r:dq:f64


# R/M 256
PATTERN : VV1 0xA8 VL256 V66 V0F38 W1 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()
OPERANDS  : REG0=YMM_R():rw:qq:f64 REG1=YMM_N():r:qq:f64    MEM0:r:qq:f64
# R/R 256
PATTERN : VV1 0xA8 VL256 V66 V0F38 W1 MOD[0b11] MOD=3 REG[rrr] RM[nnn]
OPERANDS  : REG0=YMM_R():rw:qq:f64 REG1=YMM_N():r:qq:f64 REG2=YMM_B():r:qq:f64
}
{
ICLASS    : VFMADD213PS
EXCEPTIONS: avx-type-2
CPL       : 3
CATEGORY  : VFMA
EXTENSION : FMA
ATTRIBUTES: MXCSR
# R/M 128
PATTERN : VV1 0xA8 VL128 V66 V0F38 W0 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()
OPERANDS  : REG0=XMM_R():rw:dq:f32 REG1=XMM_N():r:dq:f32 MEM0:r:dq:f32
# R/R 128
PATTERN : VV1 0xA8 VL128 V66 V0F38 W0 MOD[0b11] MOD=3 REG[rrr] RM[nnn]
OPERANDS  : REG0=XMM_R():rw:dq:f32 REG1=XMM_N():r:dq:f32 REG2=XMM_B():r:dq:f32


# R/M 256
PATTERN : VV1 0xA8 VL256 V66 V0F38 W0 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()
OPERANDS  : REG0=YMM_R():rw:qq:f32 REG1=YMM_N():r:qq:f32 MEM0:r:qq:f32
# R/R 256
PATTERN : VV1 0xA8 VL256 V66 V0F38 W0 MOD[0b11] MOD=3 REG[rrr] RM[nnn]
OPERANDS  : REG0=YMM_R():rw:qq:f32 REG1=YMM_N():r:qq:f32 REG2=YMM_B():r:qq:f32
}
{
ICLASS    : VFMADD213SD
EXCEPTIONS: avx-type-3
CPL       : 3
CATEGORY  : VFMA
EXTENSION : FMA
ATTRIBUTES: MXCSR simd_scalar
# R/M 128
PATTERN : VV1 0xA9  V66 V0F38 W1 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()
OPERANDS  : REG0=XMM_R():rw:dq:f64 REG1=XMM_N():r:q:f64     MEM0:r:q:f64
# R/R 128
PATTERN : VV1 0xA9  V66 V0F38 W1 MOD[0b11] MOD=3 REG[rrr] RM[nnn]
OPERANDS  : REG0=XMM_R():rw:dq:f64 REG1=XMM_N():r:q:f64 REG2=XMM_B():r:q:f64

}
{
ICLASS    : VFMADD213SS
EXCEPTIONS: avx-type-3
CPL       : 3
CATEGORY  : VFMA
EXTENSION : FMA
ATTRIBUTES: MXCSR simd_scalar
# R/M 128
PATTERN : VV1 0xA9  V66 V0F38 W0 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()
OPERANDS  : REG0=XMM_R():rw:dq:f32 REG1=XMM_N():r:d:f32     MEM0:r:d:f32
# R/R 128
PATTERN : VV1 0xA9  V66 V0F38 W0 MOD[0b11] MOD=3 REG[rrr] RM[nnn]
OPERANDS  : REG0=XMM_R():rw:dq:f32 REG1=XMM_N():r:d:f32 REG2=XMM_B():r:d:f32
}

{
ICLASS    : VFMADD231PD
EXCEPTIONS: avx-type-2
CPL       : 3
CATEGORY  : VFMA
EXTENSION : FMA
ATTRIBUTES: MXCSR
# R/M 128
PATTERN : VV1 0xB8 VL128 V66 V0F38 W1 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()
OPERANDS  : REG0=XMM_R():rw:dq:f64 REG1=XMM_N():r:dq:f64 MEM0:r:dq:f64
# R/R 128
PATTERN : VV1 0xB8 VL128 V66 V0F38 W1 MOD[0b11] MOD=3 REG[rrr] RM[nnn]
OPERANDS  : REG0=XMM_R():rw:dq:f64 REG1=XMM_N():r:dq:f64 REG2=XMM_B():r:dq:f64


# R/M 256
PATTERN : VV1 0xB8 VL256 V66 V0F38 W1 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()
OPERANDS  : REG0=YMM_R():rw:qq:f64 REG1=YMM_N():r:qq:f64 MEM0:r:qq:f64
# R/R 256
PATTERN : VV1 0xB8 VL256 V66 V0F38 W1 MOD[0b11] MOD=3 REG[rrr] RM[nnn]
OPERANDS  : REG0=YMM_R():rw:qq:f64 REG1=YMM_N():r:qq:f64 REG2=YMM_B():r:qq:f64

}
{
ICLASS    : VFMADD231PS
EXCEPTIONS: avx-type-2
CPL       : 3
CATEGORY  : VFMA
EXTENSION : FMA
ATTRIBUTES: MXCSR
# R/M 128
PATTERN : VV1 0xB8 VL128 V66 V0F38 W0 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()
OPERANDS  : REG0=XMM_R():rw:dq:f32 REG1=XMM_N():r:dq:f32 MEM0:r:dq:f32
# R/R 128
PATTERN : VV1 0xB8 VL128 V66 V0F38 W0 MOD[0b11] MOD=3 REG[rrr] RM[nnn]
OPERANDS  : REG0=XMM_R():rw:dq:f32 REG1=XMM_N():r:dq:f32 REG2=XMM_B():r:dq:f32

# R/M 256
PATTERN : VV1 0xB8 VL256 V66 V0F38 W0 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()
OPERANDS  : REG0=YMM_R():rw:qq:f32 REG1=YMM_N():r:qq:f32 MEM0:r:qq:f32
# R/R 256
PATTERN : VV1 0xB8 VL256 V66 V0F38 W0 MOD[0b11] MOD=3 REG[rrr] RM[nnn]
OPERANDS  : REG0=YMM_R():rw:qq:f32 REG1=YMM_N():r:qq:f32 REG2=YMM_B():r:qq:f32

}
{
ICLASS    : VFMADD231SD
EXCEPTIONS: avx-type-3
CPL       : 3
CATEGORY  : VFMA
EXTENSION : FMA
ATTRIBUTES: MXCSR simd_scalar
# R/M 128
PATTERN : VV1 0xB9 V66 V0F38 W1 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()
OPERANDS  : REG0=XMM_R():rw:dq:f64 REG1=XMM_N():r:q:f64 MEM0:r:q:f64
# R/R 128
PATTERN : VV1 0xB9 V66 V0F38 W1 MOD[0b11] MOD=3 REG[rrr] RM[nnn]
OPERANDS  : REG0=XMM_R():rw:dq:f64 REG1=XMM_N():r:q:f64 REG2=XMM_B():r:q:f64

}
{
ICLASS    : VFMADD231SS
EXCEPTIONS: avx-type-3
CPL       : 3
CATEGORY  : VFMA
EXTENSION : FMA
ATTRIBUTES: MXCSR simd_scalar
# R/M 128
PATTERN : VV1 0xB9 V66 V0F38 W0 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()
OPERANDS  : REG0=XMM_R():rw:dq:f32 REG1=XMM_N():r:d:f32 MEM0:r:d:f32
# R/R 128
PATTERN : VV1 0xB9 V66 V0F38 W0 MOD[0b11] MOD=3 REG[rrr] RM[nnn]
OPERANDS  : REG0=XMM_R():rw:dq:f32 REG1=XMM_N():r:d:f32 REG2=XMM_B():r:d:f32

}


###################################################
{
ICLASS    : VFMADDSUB132PD
EXCEPTIONS: avx-type-2
CPL       : 3
CATEGORY  : VFMA
EXTENSION : FMA
ATTRIBUTES: MXCSR
# R/M 128
PATTERN : VV1 0x96 VL128 V66 V0F38 W1 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()
OPERANDS  : REG0=XMM_R():rw:dq:f64 REG1=XMM_N():r:dq:f64 MEM0:r:dq:f64
# R/R 128
PATTERN : VV1 0x96 VL128 V66 V0F38 W1 MOD[0b11] MOD=3 REG[rrr] RM[nnn]
OPERANDS  : REG0=XMM_R():rw:dq:f64 REG1=XMM_N():r:dq:f64 REG2=XMM_B():r:dq:f64


# R/M 256
PATTERN : VV1 0x96 VL256 V66 V0F38 W1 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()
OPERANDS  : REG0=YMM_R():rw:qq:f64 REG1=YMM_N():r:qq:f64 MEM0:r:qq:f64
# R/R 256
PATTERN : VV1 0x96 VL256 V66 V0F38 W1 MOD[0b11] MOD=3 REG[rrr] RM[nnn]
OPERANDS  : REG0=YMM_R():rw:qq:f64 REG1=YMM_N():r:qq:f64 REG2=YMM_B():r:qq:f64
}
{
ICLASS    : VFMADDSUB213PD
EXCEPTIONS: avx-type-2
CPL       : 3
CATEGORY  : VFMA
EXTENSION : FMA
ATTRIBUTES: MXCSR
# R/M 128
PATTERN : VV1 0xA6 VL128 V66 V0F38 W1 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()
OPERANDS  : REG0=XMM_R():rw:dq:f64 REG1=XMM_N():r:dq:f64    MEM0:r:dq:f64
# R/R 128
PATTERN : VV1 0xA6 VL128 V66 V0F38 W1 MOD[0b11] MOD=3 REG[rrr] RM[nnn]
OPERANDS  : REG0=XMM_R():rw:dq:f64 REG1=XMM_N():r:dq:f64 REG2=XMM_B():r:dq:f64


# R/M 256
PATTERN : VV1 0xA6 VL256 V66 V0F38 W1 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()
OPERANDS  : REG0=YMM_R():rw:qq:f64 REG1=YMM_N():r:qq:f64    MEM0:r:qq:f64
# R/R 256
PATTERN : VV1 0xA6 VL256 V66 V0F38 W1 MOD[0b11] MOD=3 REG[rrr] RM[nnn]
OPERANDS  : REG0=YMM_R():rw:qq:f64 REG1=YMM_N():r:qq:f64 REG2=YMM_B():r:qq:f64
}
{
ICLASS    : VFMADDSUB231PD
EXCEPTIONS: avx-type-2
CPL       : 3
CATEGORY  : VFMA
EXTENSION : FMA
ATTRIBUTES: MXCSR
# R/M 128
PATTERN : VV1 0xB6 VL128 V66 V0F38 W1 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()
OPERANDS  : REG0=XMM_R():rw:dq:f64 REG1=XMM_N():r:dq:f64 MEM0:r:dq:f64
# R/R 128
PATTERN : VV1 0xB6 VL128 V66 V0F38 W1 MOD[0b11] MOD=3 REG[rrr] RM[nnn]
OPERANDS  : REG0=XMM_R():rw:dq:f64 REG1=XMM_N():r:dq:f64 REG2=XMM_B():r:dq:f64


# R/M 256
PATTERN : VV1 0xB6 VL256 V66 V0F38 W1 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()
OPERANDS  : REG0=YMM_R():rw:qq:f64 REG1=YMM_N():r:qq:f64 MEM0:r:qq:f64
# R/R 256
PATTERN : VV1 0xB6 VL256 V66 V0F38 W1 MOD[0b11] MOD=3 REG[rrr] RM[nnn]
OPERANDS  : REG0=YMM_R():rw:qq:f64 REG1=YMM_N():r:qq:f64 REG2=YMM_B():r:qq:f64

}

{
ICLASS    : VFMADDSUB132PS
EXCEPTIONS: avx-type-2
CPL       : 3
CATEGORY  : VFMA
EXTENSION : FMA
ATTRIBUTES: MXCSR
# R/M 128
PATTERN : VV1 0x96 VL128 V66 V0F38 W0 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()
OPERANDS  : REG0=XMM_R():rw:dq:f32 REG1=XMM_N():r:dq:f32 MEM0:r:dq:f32
# R/R 128
PATTERN : VV1 0x96 VL128 V66 V0F38 W0 MOD[0b11] MOD=3 REG[rrr] RM[nnn]
OPERANDS  : REG0=XMM_R():rw:dq:f32 REG1=XMM_N():r:dq:f32 REG2=XMM_B():r:dq:f32


# R/M 256
PATTERN : VV1 0x96 VL256 V66 V0F38 W0 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()
OPERANDS  : REG0=YMM_R():rw:qq:f32 REG1=YMM_N():r:qq:f32 MEM0:r:qq:f32
# R/R 256
PATTERN : VV1 0x96 VL256 V66 V0F38 W0 MOD[0b11] MOD=3 REG[rrr] RM[nnn]
OPERANDS  : REG0=YMM_R():rw:qq:f32 REG1=YMM_N():r:qq:f32 REG2=YMM_B():r:qq:f32
}
{
ICLASS    : VFMADDSUB213PS
EXCEPTIONS: avx-type-2
CPL       : 3
CATEGORY  : VFMA
EXTENSION : FMA
ATTRIBUTES: MXCSR
# R/M 128
PATTERN : VV1 0xA6 VL128 V66 V0F38 W0 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()
OPERANDS  : REG0=XMM_R():rw:dq:f32 REG1=XMM_N():r:dq:f32 MEM0:r:dq:f32
# R/R 128
PATTERN : VV1 0xA6 VL128 V66 V0F38 W0 MOD[0b11] MOD=3 REG[rrr] RM[nnn]
OPERANDS  : REG0=XMM_R():rw:dq:f32 REG1=XMM_N():r:dq:f32 REG2=XMM_B():r:dq:f32


# R/M 256
PATTERN : VV1 0xA6 VL256 V66 V0F38 W0 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()
OPERANDS  : REG0=YMM_R():rw:qq:f32 REG1=YMM_N():r:qq:f32 MEM0:r:qq:f32
# R/R 256
PATTERN : VV1 0xA6 VL256 V66 V0F38 W0 MOD[0b11] MOD=3 REG[rrr] RM[nnn]
OPERANDS  : REG0=YMM_R():rw:qq:f32 REG1=YMM_N():r:qq:f32 REG2=YMM_B():r:qq:f32
}
{
ICLASS    : VFMADDSUB231PS
EXCEPTIONS: avx-type-2
CPL       : 3
CATEGORY  : VFMA
EXTENSION : FMA
ATTRIBUTES: MXCSR
# R/M 128
PATTERN : VV1 0xB6 VL128 V66 V0F38 W0 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()
OPERANDS  : REG0=XMM_R():rw:dq:f32 REG1=XMM_N():r:dq:f32 MEM0:r:dq:f32
# R/R 128
PATTERN : VV1 0xB6 VL128 V66 V0F38 W0 MOD[0b11] MOD=3 REG[rrr] RM[nnn]
OPERANDS  : REG0=XMM_R():rw:dq:f32 REG1=XMM_N():r:dq:f32 REG2=XMM_B():r:dq:f32

# R/M 256
PATTERN : VV1 0xB6 VL256 V66 V0F38 W0 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()
OPERANDS  : REG0=YMM_R():rw:qq:f32 REG1=YMM_N():r:qq:f32 MEM0:r:qq:f32
# R/R 256
PATTERN : VV1 0xB6 VL256 V66 V0F38 W0 MOD[0b11] MOD=3 REG[rrr] RM[nnn]
OPERANDS  : REG0=YMM_R():rw:qq:f32 REG1=YMM_N():r:qq:f32 REG2=YMM_B():r:qq:f32

}
###################################################

{
ICLASS    : VFMSUBADD132PD
EXCEPTIONS: avx-type-2
CPL       : 3
CATEGORY  : VFMA
EXTENSION : FMA
ATTRIBUTES: MXCSR
# R/M 128
PATTERN : VV1 0x97 VL128 V66 V0F38 W1 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()
OPERANDS  : REG0=XMM_R():rw:dq:f64 REG1=XMM_N():r:dq:f64 MEM0:r:dq:f64
# R/R 128
PATTERN : VV1 0x97 VL128 V66 V0F38 W1 MOD[0b11] MOD=3 REG[rrr] RM[nnn]
OPERANDS  : REG0=XMM_R():rw:dq:f64 REG1=XMM_N():r:dq:f64 REG2=XMM_B():r:dq:f64


# R/M 256
PATTERN : VV1 0x97 VL256 V66 V0F38 W1 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()
OPERANDS  : REG0=YMM_R():rw:qq:f64 REG1=YMM_N():r:qq:f64 MEM0:r:qq:f64
# R/R 256
PATTERN : VV1 0x97 VL256 V66 V0F38 W1 MOD[0b11] MOD=3 REG[rrr] RM[nnn]
OPERANDS  : REG0=YMM_R():rw:qq:f64 REG1=YMM_N():r:qq:f64 REG2=YMM_B():r:qq:f64
}
{
ICLASS    : VFMSUBADD213PD
EXCEPTIONS: avx-type-2
CPL       : 3
CATEGORY  : VFMA
EXTENSION : FMA
ATTRIBUTES: MXCSR
# R/M 128
PATTERN : VV1 0xA7 VL128 V66 V0F38 W1 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()
OPERANDS  : REG0=XMM_R():rw:dq:f64 REG1=XMM_N():r:dq:f64    MEM0:r:dq:f64
# R/R 128
PATTERN : VV1 0xA7 VL128 V66 V0F38 W1 MOD[0b11] MOD=3 REG[rrr] RM[nnn]
OPERANDS  : REG0=XMM_R():rw:dq:f64 REG1=XMM_N():r:dq:f64 REG2=XMM_B():r:dq:f64


# R/M 256
PATTERN : VV1 0xA7 VL256 V66 V0F38 W1 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()
OPERANDS  : REG0=YMM_R():rw:qq:f64 REG1=YMM_N():r:qq:f64    MEM0:r:qq:f64
# R/R 256
PATTERN : VV1 0xA7 VL256 V66 V0F38 W1 MOD[0b11] MOD=3 REG[rrr] RM[nnn]
OPERANDS  : REG0=YMM_R():rw:qq:f64 REG1=YMM_N():r:qq:f64 REG2=YMM_B():r:qq:f64
}
{
ICLASS    : VFMSUBADD231PD
EXCEPTIONS: avx-type-2
CPL       : 3
CATEGORY  : VFMA
EXTENSION : FMA
ATTRIBUTES: MXCSR
# R/M 128
PATTERN : VV1 0xB7 VL128 V66 V0F38 W1 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()
OPERANDS  : REG0=XMM_R():rw:dq:f64 REG1=XMM_N():r:dq:f64 MEM0:r:dq:f64
# R/R 128
PATTERN : VV1 0xB7 VL128 V66 V0F38 W1 MOD[0b11] MOD=3 REG[rrr] RM[nnn]
OPERANDS  : REG0=XMM_R():rw:dq:f64 REG1=XMM_N():r:dq:f64 REG2=XMM_B():r:dq:f64


# R/M 256
PATTERN : VV1 0xB7 VL256 V66 V0F38 W1 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()
OPERANDS  : REG0=YMM_R():rw:qq:f64 REG1=YMM_N():r:qq:f64 MEM0:r:qq:f64
# R/R 256
PATTERN : VV1 0xB7 VL256 V66 V0F38 W1 MOD[0b11] MOD=3 REG[rrr] RM[nnn]
OPERANDS  : REG0=YMM_R():rw:qq:f64 REG1=YMM_N():r:qq:f64 REG2=YMM_B():r:qq:f64

}

{
ICLASS    : VFMSUBADD132PS
EXCEPTIONS: avx-type-2
CPL       : 3
CATEGORY  : VFMA
EXTENSION : FMA
ATTRIBUTES: MXCSR
# R/M 128
PATTERN : VV1 0x97 VL128 V66 V0F38 W0 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()
OPERANDS  : REG0=XMM_R():rw:dq:f32 REG1=XMM_N():r:dq:f32 MEM0:r:dq:f32
# R/R 128
PATTERN : VV1 0x97 VL128 V66 V0F38 W0 MOD[0b11] MOD=3 REG[rrr] RM[nnn]
OPERANDS  : REG0=XMM_R():rw:dq:f32 REG1=XMM_N():r:dq:f32 REG2=XMM_B():r:dq:f32


# R/M 256
PATTERN : VV1 0x97 VL256 V66 V0F38 W0 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()
OPERANDS  : REG0=YMM_R():rw:qq:f32 REG1=YMM_N():r:qq:f32 MEM0:r:qq:f32
# R/R 256
PATTERN : VV1 0x97 VL256 V66 V0F38 W0 MOD[0b11] MOD=3 REG[rrr] RM[nnn]
OPERANDS  : REG0=YMM_R():rw:qq:f32 REG1=YMM_N():r:qq:f32 REG2=YMM_B():r:qq:f32
}
{
ICLASS    : VFMSUBADD213PS
EXCEPTIONS: avx-type-2
CPL       : 3
CATEGORY  : VFMA
EXTENSION : FMA
ATTRIBUTES: MXCSR
# R/M 128
PATTERN : VV1 0xA7 VL128 V66 V0F38 W0 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()
OPERANDS  : REG0=XMM_R():rw:dq:f32 REG1=XMM_N():r:dq:f32 MEM0:r:dq:f32
# R/R 128
PATTERN : VV1 0xA7 VL128 V66 V0F38 W0 MOD[0b11] MOD=3 REG[rrr] RM[nnn]
OPERANDS  : REG0=XMM_R():rw:dq:f32 REG1=XMM_N():r:dq:f32 REG2=XMM_B():r:dq:f32


# R/M 256
PATTERN : VV1 0xA7 VL256 V66 V0F38 W0 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()
OPERANDS  : REG0=YMM_R():rw:qq:f32 REG1=YMM_N():r:qq:f32 MEM0:r:qq:f32
# R/R 256
PATTERN : VV1 0xA7 VL256 V66 V0F38 W0 MOD[0b11] MOD=3 REG[rrr] RM[nnn]
OPERANDS  : REG0=YMM_R():rw:qq:f32 REG1=YMM_N():r:qq:f32 REG2=YMM_B():r:qq:f32
}
{
ICLASS    : VFMSUBADD231PS
EXCEPTIONS: avx-type-2
CPL       : 3
CATEGORY  : VFMA
EXTENSION : FMA
ATTRIBUTES: MXCSR
# R/M 128
PATTERN : VV1 0xB7 VL128 V66 V0F38 W0 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()
OPERANDS  : REG0=XMM_R():rw:dq:f32 REG1=XMM_N():r:dq:f32 MEM0:r:dq:f32
# R/R 128
PATTERN : VV1 0xB7 VL128 V66 V0F38 W0 MOD[0b11] MOD=3 REG[rrr] RM[nnn]
OPERANDS  : REG0=XMM_R():rw:dq:f32 REG1=XMM_N():r:dq:f32 REG2=XMM_B():r:dq:f32

# R/M 256
PATTERN : VV1 0xB7 VL256 V66 V0F38 W0 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()
OPERANDS  : REG0=YMM_R():rw:qq:f32 REG1=YMM_N():r:qq:f32 MEM0:r:qq:f32
# R/R 256
PATTERN : VV1 0xB7 VL256 V66 V0F38 W0 MOD[0b11] MOD=3 REG[rrr] RM[nnn]
OPERANDS  : REG0=YMM_R():rw:qq:f32 REG1=YMM_N():r:qq:f32 REG2=YMM_B():r:qq:f32

}


###################################################

{
ICLASS    : VFMSUB132PD
EXCEPTIONS: avx-type-2
CPL       : 3
CATEGORY  : VFMA
EXTENSION : FMA
ATTRIBUTES: MXCSR
# R/M 128
PATTERN : VV1 0x9A VL128 V66 V0F38 W1 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()
OPERANDS  : REG0=XMM_R():rw:dq:f64 REG1=XMM_N():r:dq:f64 MEM0:r:dq:f64
# R/R 128
PATTERN : VV1 0x9A VL128 V66 V0F38 W1 MOD[0b11] MOD=3 REG[rrr] RM[nnn]
OPERANDS  : REG0=XMM_R():rw:dq:f64 REG1=XMM_N():r:dq:f64 REG2=XMM_B():r:dq:f64


# R/M 256
PATTERN : VV1 0x9A VL256 V66 V0F38 W1 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()
OPERANDS  : REG0=YMM_R():rw:qq:f64 REG1=YMM_N():r:qq:f64 MEM0:r:qq:f64
# R/R 256
PATTERN : VV1 0x9A VL256 V66 V0F38 W1 MOD[0b11] MOD=3 REG[rrr] RM[nnn]
OPERANDS  : REG0=YMM_R():rw:qq:f64 REG1=YMM_N():r:qq:f64 REG2=YMM_B():r:qq:f64
}
{
ICLASS    : VFMSUB132PS
EXCEPTIONS: avx-type-2
CPL       : 3
CATEGORY  : VFMA
EXTENSION : FMA
ATTRIBUTES: MXCSR
# R/M 128
PATTERN : VV1 0x9A VL128 V66 V0F38 W0 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()
OPERANDS  : REG0=XMM_R():rw:dq:f32 REG1=XMM_N():r:dq:f32 MEM0:r:dq:f32
# R/R 128
PATTERN : VV1 0x9A VL128 V66 V0F38 W0 MOD[0b11] MOD=3 REG[rrr] RM[nnn]
OPERANDS  : REG0=XMM_R():rw:dq:f32 REG1=XMM_N():r:dq:f32 REG2=XMM_B():r:dq:f32


# R/M 256
PATTERN : VV1 0x9A VL256 V66 V0F38 W0 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()
OPERANDS  : REG0=YMM_R():rw:qq:f32 REG1=YMM_N():r:qq:f32 MEM0:r:qq:f32
# R/R 256
PATTERN : VV1 0x9A VL256 V66 V0F38 W0 MOD[0b11] MOD=3 REG[rrr] RM[nnn]
OPERANDS  : REG0=YMM_R():rw:qq:f32 REG1=YMM_N():r:qq:f32 REG2=YMM_B():r:qq:f32
}
{
ICLASS    : VFMSUB132SD
EXCEPTIONS: avx-type-3
CPL       : 3
CATEGORY  : VFMA
EXTENSION : FMA
ATTRIBUTES: MXCSR simd_scalar
# R/M 128
PATTERN : VV1 0x9B V66 V0F38 W1 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()
OPERANDS  : REG0=XMM_R():rw:dq:f64 REG1=XMM_N():r:q:f64 MEM0:r:q:f64
# R/R 128
PATTERN : VV1 0x9B V66 V0F38 W1 MOD[0b11] MOD=3 REG[rrr] RM[nnn]
OPERANDS  : REG0=XMM_R():rw:dq:f64 REG1=XMM_N():r:q:f64 REG2=XMM_B():r:q:f64
}
{
ICLASS    : VFMSUB132SS
EXCEPTIONS: avx-type-3
CPL       : 3
CATEGORY  : VFMA
EXTENSION : FMA
ATTRIBUTES: MXCSR simd_scalar
# R/M 128
PATTERN : VV1 0x9B  V66 V0F38 W0 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()
OPERANDS  : REG0=XMM_R():rw:dq:f32 REG1=XMM_N():r:d:f32 MEM0:r:d:f32
# R/R 128
PATTERN : VV1 0x9B  V66 V0F38 W0 MOD[0b11] MOD=3 REG[rrr] RM[nnn]
OPERANDS  : REG0=XMM_R():rw:dq:f32 REG1=XMM_N():r:d:f32 REG2=XMM_B():r:d:f32

}

{
ICLASS    : VFMSUB213PD
EXCEPTIONS: avx-type-2
CPL       : 3
CATEGORY  : VFMA
EXTENSION : FMA
ATTRIBUTES: MXCSR
# R/M 128
PATTERN : VV1 0xAA VL128 V66 V0F38 W1 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()
OPERANDS  : REG0=XMM_R():rw:dq:f64 REG1=XMM_N():r:dq:f64    MEM0:r:dq:f64
# R/R 128
PATTERN : VV1 0xAA VL128 V66 V0F38 W1 MOD[0b11] MOD=3 REG[rrr] RM[nnn]
OPERANDS  : REG0=XMM_R():rw:dq:f64 REG1=XMM_N():r:dq:f64 REG2=XMM_B():r:dq:f64


# R/M 256
PATTERN : VV1 0xAA VL256 V66 V0F38 W1 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()
OPERANDS  : REG0=YMM_R():rw:qq:f64 REG1=YMM_N():r:qq:f64    MEM0:r:qq:f64
# R/R 256
PATTERN : VV1 0xAA VL256 V66 V0F38 W1 MOD[0b11] MOD=3 REG[rrr] RM[nnn]
OPERANDS  : REG0=YMM_R():rw:qq:f64 REG1=YMM_N():r:qq:f64 REG2=YMM_B():r:qq:f64
}
{
ICLASS    : VFMSUB213PS
EXCEPTIONS: avx-type-2
CPL       : 3
CATEGORY  : VFMA
EXTENSION : FMA
ATTRIBUTES: MXCSR
# R/M 128
PATTERN : VV1 0xAA VL128 V66 V0F38 W0 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()
OPERANDS  : REG0=XMM_R():rw:dq:f32 REG1=XMM_N():r:dq:f32 MEM0:r:dq:f32
# R/R 128
PATTERN : VV1 0xAA VL128 V66 V0F38 W0 MOD[0b11] MOD=3 REG[rrr] RM[nnn]
OPERANDS  : REG0=XMM_R():rw:dq:f32 REG1=XMM_N():r:dq:f32 REG2=XMM_B():r:dq:f32


# R/M 256
PATTERN : VV1 0xAA VL256 V66 V0F38 W0 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()
OPERANDS  : REG0=YMM_R():rw:qq:f32 REG1=YMM_N():r:qq:f32 MEM0:r:qq:f32
# R/R 256
PATTERN : VV1 0xAA VL256 V66 V0F38 W0 MOD[0b11] MOD=3 REG[rrr] RM[nnn]
OPERANDS  : REG0=YMM_R():rw:qq:f32 REG1=YMM_N():r:qq:f32 REG2=YMM_B():r:qq:f32
}
{
ICLASS    : VFMSUB213SD
EXCEPTIONS: avx-type-3
CPL       : 3
CATEGORY  : VFMA
EXTENSION : FMA
ATTRIBUTES: MXCSR simd_scalar
# R/M 128
PATTERN : VV1 0xAB  V66 V0F38 W1 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()
OPERANDS  : REG0=XMM_R():rw:dq:f64 REG1=XMM_N():r:q:f64     MEM0:r:q:f64
# R/R 128
PATTERN : VV1 0xAB  V66 V0F38 W1 MOD[0b11] MOD=3 REG[rrr] RM[nnn]
OPERANDS  : REG0=XMM_R():rw:dq:f64 REG1=XMM_N():r:q:f64 REG2=XMM_B():r:q:f64

}
{
ICLASS    : VFMSUB213SS
EXCEPTIONS: avx-type-3
CPL       : 3
CATEGORY  : VFMA
EXTENSION : FMA
ATTRIBUTES: MXCSR simd_scalar
# R/M 128
PATTERN : VV1 0xAB  V66 V0F38 W0 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()
OPERANDS  : REG0=XMM_R():rw:dq:f32 REG1=XMM_N():r:d:f32     MEM0:r:d:f32
# R/R 128
PATTERN : VV1 0xAB  V66 V0F38 W0 MOD[0b11] MOD=3 REG[rrr] RM[nnn]
OPERANDS  : REG0=XMM_R():rw:dq:f32 REG1=XMM_N():r:d:f32 REG2=XMM_B():r:d:f32
}

{
ICLASS    : VFMSUB231PD
EXCEPTIONS: avx-type-2
CPL       : 3
CATEGORY  : VFMA
EXTENSION : FMA
ATTRIBUTES: MXCSR
# R/M 128
PATTERN : VV1 0xBA VL128 V66 V0F38 W1 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()
OPERANDS  : REG0=XMM_R():rw:dq:f64 REG1=XMM_N():r:dq:f64 MEM0:r:dq:f64
# R/R 128
PATTERN : VV1 0xBA VL128 V66 V0F38 W1 MOD[0b11] MOD=3 REG[rrr] RM[nnn]
OPERANDS  : REG0=XMM_R():rw:dq:f64 REG1=XMM_N():r:dq:f64 REG2=XMM_B():r:dq:f64


# R/M 256
PATTERN : VV1 0xBA VL256 V66 V0F38 W1 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()
OPERANDS  : REG0=YMM_R():rw:qq:f64 REG1=YMM_N():r:qq:f64 MEM0:r:qq:f64
# R/R 256
PATTERN : VV1 0xBA VL256 V66 V0F38 W1 MOD[0b11] MOD=3 REG[rrr] RM[nnn]
OPERANDS  : REG0=YMM_R():rw:qq:f64 REG1=YMM_N():r:qq:f64 REG2=YMM_B():r:qq:f64

}
{
ICLASS    : VFMSUB231PS
EXCEPTIONS: avx-type-2
CPL       : 3
CATEGORY  : VFMA
EXTENSION : FMA
ATTRIBUTES: MXCSR
# R/M 128
PATTERN : VV1 0xBA VL128 V66 V0F38 W0 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()
OPERANDS  : REG0=XMM_R():rw:dq:f32 REG1=XMM_N():r:dq:f32 MEM0:r:dq:f32
# R/R 128
PATTERN : VV1 0xBA VL128 V66 V0F38 W0 MOD[0b11] MOD=3 REG[rrr] RM[nnn]
OPERANDS  : REG0=XMM_R():rw:dq:f32 REG1=XMM_N():r:dq:f32 REG2=XMM_B():r:dq:f32

# R/M 256
PATTERN : VV1 0xBA VL256 V66 V0F38 W0 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()
OPERANDS  : REG0=YMM_R():rw:qq:f32 REG1=YMM_N():r:qq:f32 MEM0:r:qq:f32
# R/R 256
PATTERN : VV1 0xBA VL256 V66 V0F38 W0 MOD[0b11] MOD=3 REG[rrr] RM[nnn]
OPERANDS  : REG0=YMM_R():rw:qq:f32 REG1=YMM_N():r:qq:f32 REG2=YMM_B():r:qq:f32

}
{
ICLASS    : VFMSUB231SD
EXCEPTIONS: avx-type-3
CPL       : 3
CATEGORY  : VFMA
EXTENSION : FMA
ATTRIBUTES: MXCSR simd_scalar
# R/M 128
PATTERN : VV1 0xBB V66 V0F38 W1 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()
OPERANDS  : REG0=XMM_R():rw:dq:f64 REG1=XMM_N():r:q:f64 MEM0:r:q:f64
# R/R 128
PATTERN : VV1 0xBB V66 V0F38 W1 MOD[0b11] MOD=3 REG[rrr] RM[nnn]
OPERANDS  : REG0=XMM_R():rw:dq:f64 REG1=XMM_N():r:q:f64 REG2=XMM_B():r:q:f64

}
{
ICLASS    : VFMSUB231SS
EXCEPTIONS: avx-type-3
CPL       : 3
CATEGORY  : VFMA
EXTENSION : FMA
ATTRIBUTES: MXCSR simd_scalar
# R/M 128
PATTERN : VV1 0xBB V66 V0F38 W0 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()
OPERANDS  : REG0=XMM_R():rw:dq:f32 REG1=XMM_N():r:d:f32 MEM0:r:d:f32
# R/R 128
PATTERN : VV1 0xBB V66 V0F38 W0 MOD[0b11] MOD=3 REG[rrr] RM[nnn]
OPERANDS  : REG0=XMM_R():rw:dq:f32 REG1=XMM_N():r:d:f32 REG2=XMM_B():r:d:f32

}

###################################################


{
ICLASS    : VFNMADD132PD
EXCEPTIONS: avx-type-2
CPL       : 3
CATEGORY  : VFMA
EXTENSION : FMA
ATTRIBUTES: MXCSR
# R/M 128
PATTERN : VV1 0x9C VL128 V66 V0F38 W1 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()
OPERANDS  : REG0=XMM_R():rw:dq:f64 REG1=XMM_N():r:dq:f64 MEM0:r:dq:f64
# R/R 128
PATTERN : VV1 0x9C VL128 V66 V0F38 W1 MOD[0b11] MOD=3 REG[rrr] RM[nnn]
OPERANDS  : REG0=XMM_R():rw:dq:f64 REG1=XMM_N():r:dq:f64 REG2=XMM_B():r:dq:f64


# R/M 256
PATTERN : VV1 0x9C VL256 V66 V0F38 W1 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()
OPERANDS  : REG0=YMM_R():rw:qq:f64 REG1=YMM_N():r:qq:f64 MEM0:r:qq:f64
# R/R 256
PATTERN : VV1 0x9C VL256 V66 V0F38 W1 MOD[0b11] MOD=3 REG[rrr] RM[nnn]
OPERANDS  : REG0=YMM_R():rw:qq:f64 REG1=YMM_N():r:qq:f64 REG2=YMM_B():r:qq:f64
}
{
ICLASS    : VFNMADD132PS
EXCEPTIONS: avx-type-2
CPL       : 3
CATEGORY  : VFMA
EXTENSION : FMA
ATTRIBUTES: MXCSR
# R/M 128
PATTERN : VV1 0x9C VL128 V66 V0F38 W0 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()
OPERANDS  : REG0=XMM_R():rw:dq:f32 REG1=XMM_N():r:dq:f32 MEM0:r:dq:f32
# R/R 128
PATTERN : VV1 0x9C VL128 V66 V0F38 W0 MOD[0b11] MOD=3 REG[rrr] RM[nnn]
OPERANDS  : REG0=XMM_R():rw:dq:f32 REG1=XMM_N():r:dq:f32 REG2=XMM_B():r:dq:f32


# R/M 256
PATTERN : VV1 0x9C VL256 V66 V0F38 W0 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()
OPERANDS  : REG0=YMM_R():rw:qq:f32 REG1=YMM_N():r:qq:f32 MEM0:r:qq:f32
# R/R 256
PATTERN : VV1 0x9C VL256 V66 V0F38 W0 MOD[0b11] MOD=3 REG[rrr] RM[nnn]
OPERANDS  : REG0=YMM_R():rw:qq:f32 REG1=YMM_N():r:qq:f32 REG2=YMM_B():r:qq:f32
}
{
ICLASS    : VFNMADD132SD
EXCEPTIONS: avx-type-3
CPL       : 3
CATEGORY  : VFMA
EXTENSION : FMA
ATTRIBUTES: MXCSR simd_scalar
# R/M 128
PATTERN : VV1 0x9D V66 V0F38 W1 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()
OPERANDS  : REG0=XMM_R():rw:dq:f64 REG1=XMM_N():r:q:f64 MEM0:r:q:f64
# R/R 128
PATTERN : VV1 0x9D V66 V0F38 W1 MOD[0b11] MOD=3 REG[rrr] RM[nnn]
OPERANDS  : REG0=XMM_R():rw:dq:f64 REG1=XMM_N():r:q:f64 REG2=XMM_B():r:q:f64
}
{
ICLASS    : VFNMADD132SS
EXCEPTIONS: avx-type-3
CPL       : 3
CATEGORY  : VFMA
EXTENSION : FMA
ATTRIBUTES: MXCSR simd_scalar
# R/M 128
PATTERN : VV1 0x9D  V66 V0F38 W0 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()
OPERANDS  : REG0=XMM_R():rw:dq:f32 REG1=XMM_N():r:d:f32 MEM0:r:d:f32
# R/R 128
PATTERN : VV1 0x9D  V66 V0F38 W0 MOD[0b11] MOD=3 REG[rrr] RM[nnn]
OPERANDS  : REG0=XMM_R():rw:dq:f32 REG1=XMM_N():r:d:f32 REG2=XMM_B():r:d:f32

}

{
ICLASS    : VFNMADD213PD
EXCEPTIONS: avx-type-2
CPL       : 3
CATEGORY  : VFMA
EXTENSION : FMA
ATTRIBUTES: MXCSR
# R/M 128
PATTERN : VV1 0xAC VL128 V66 V0F38 W1 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()
OPERANDS  : REG0=XMM_R():rw:dq:f64 REG1=XMM_N():r:dq:f64    MEM0:r:dq:f64
# R/R 128
PATTERN : VV1 0xAC VL128 V66 V0F38 W1 MOD[0b11] MOD=3 REG[rrr] RM[nnn]
OPERANDS  : REG0=XMM_R():rw:dq:f64 REG1=XMM_N():r:dq:f64 REG2=XMM_B():r:dq:f64


# R/M 256
PATTERN : VV1 0xAC VL256 V66 V0F38 W1 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()
OPERANDS  : REG0=YMM_R():rw:qq:f64 REG1=YMM_N():r:qq:f64    MEM0:r:qq:f64
# R/R 256
PATTERN : VV1 0xAC VL256 V66 V0F38 W1 MOD[0b11] MOD=3 REG[rrr] RM[nnn]
OPERANDS  : REG0=YMM_R():rw:qq:f64 REG1=YMM_N():r:qq:f64 REG2=YMM_B():r:qq:f64
}
{
ICLASS    : VFNMADD213PS
EXCEPTIONS: avx-type-2
CPL       : 3
CATEGORY  : VFMA
EXTENSION : FMA
ATTRIBUTES: MXCSR
# R/M 128
PATTERN : VV1 0xAC VL128 V66 V0F38 W0 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()
OPERANDS  : REG0=XMM_R():rw:dq:f32 REG1=XMM_N():r:dq:f32 MEM0:r:dq:f32
# R/R 128
PATTERN : VV1 0xAC VL128 V66 V0F38 W0 MOD[0b11] MOD=3 REG[rrr] RM[nnn]
OPERANDS  : REG0=XMM_R():rw:dq:f32 REG1=XMM_N():r:dq:f32 REG2=XMM_B():r:dq:f32


# R/M 256
PATTERN : VV1 0xAC VL256 V66 V0F38 W0 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()
OPERANDS  : REG0=YMM_R():rw:qq:f32 REG1=YMM_N():r:qq:f32 MEM0:r:qq:f32
# R/R 256
PATTERN : VV1 0xAC VL256 V66 V0F38 W0 MOD[0b11] MOD=3 REG[rrr] RM[nnn]
OPERANDS  : REG0=YMM_R():rw:qq:f32 REG1=YMM_N():r:qq:f32 REG2=YMM_B():r:qq:f32
}
{
ICLASS    : VFNMADD213SD
EXCEPTIONS: avx-type-3
CPL       : 3
CATEGORY  : VFMA
EXTENSION : FMA
ATTRIBUTES: MXCSR simd_scalar
# R/M 128
PATTERN : VV1 0xAD  V66 V0F38 W1 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()
OPERANDS  : REG0=XMM_R():rw:dq:f64 REG1=XMM_N():r:q:f64     MEM0:r:q:f64
# R/R 128
PATTERN : VV1 0xAD  V66 V0F38 W1 MOD[0b11] MOD=3 REG[rrr] RM[nnn]
OPERANDS  : REG0=XMM_R():rw:dq:f64 REG1=XMM_N():r:q:f64 REG2=XMM_B():r:q:f64

}
{
ICLASS    : VFNMADD213SS
EXCEPTIONS: avx-type-3
CPL       : 3
CATEGORY  : VFMA
EXTENSION : FMA
ATTRIBUTES: MXCSR simd_scalar
# R/M 128
PATTERN : VV1 0xAD  V66 V0F38 W0 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()
OPERANDS  : REG0=XMM_R():rw:dq:f32 REG1=XMM_N():r:d:f32     MEM0:r:d:f32
# R/R 128
PATTERN : VV1 0xAD  V66 V0F38 W0 MOD[0b11] MOD=3 REG[rrr] RM[nnn]
OPERANDS  : REG0=XMM_R():rw:dq:f32 REG1=XMM_N():r:d:f32 REG2=XMM_B():r:d:f32
}

{
ICLASS    : VFNMADD231PD
EXCEPTIONS: avx-type-2
CPL       : 3
CATEGORY  : VFMA
EXTENSION : FMA
ATTRIBUTES: MXCSR
# R/M 128
PATTERN : VV1 0xBC VL128 V66 V0F38 W1 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()
OPERANDS  : REG0=XMM_R():rw:dq:f64 REG1=XMM_N():r:dq:f64 MEM0:r:dq:f64
# R/R 128
PATTERN : VV1 0xBC VL128 V66 V0F38 W1 MOD[0b11] MOD=3 REG[rrr] RM[nnn]
OPERANDS  : REG0=XMM_R():rw:dq:f64 REG1=XMM_N():r:dq:f64 REG2=XMM_B():r:dq:f64


# R/M 256
PATTERN : VV1 0xBC VL256 V66 V0F38 W1 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()
OPERANDS  : REG0=YMM_R():rw:qq:f64 REG1=YMM_N():r:qq:f64 MEM0:r:qq:f64
# R/R 256
PATTERN : VV1 0xBC VL256 V66 V0F38 W1 MOD[0b11] MOD=3 REG[rrr] RM[nnn]
OPERANDS  : REG0=YMM_R():rw:qq:f64 REG1=YMM_N():r:qq:f64 REG2=YMM_B():r:qq:f64

}
{
ICLASS    : VFNMADD231PS
EXCEPTIONS: avx-type-2
CPL       : 3
CATEGORY  : VFMA
EXTENSION : FMA
ATTRIBUTES: MXCSR
# R/M 128
PATTERN : VV1 0xBC VL128 V66 V0F38 W0 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()
OPERANDS  : REG0=XMM_R():rw:dq:f32 REG1=XMM_N():r:dq:f32 MEM0:r:dq:f32
# R/R 128
PATTERN : VV1 0xBC VL128 V66 V0F38 W0 MOD[0b11] MOD=3 REG[rrr] RM[nnn]
OPERANDS  : REG0=XMM_R():rw:dq:f32 REG1=XMM_N():r:dq:f32 REG2=XMM_B():r:dq:f32

# R/M 256
PATTERN : VV1 0xBC VL256 V66 V0F38 W0 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()
OPERANDS  : REG0=YMM_R():rw:qq:f32 REG1=YMM_N():r:qq:f32 MEM0:r:qq:f32
# R/R 256
PATTERN : VV1 0xBC VL256 V66 V0F38 W0 MOD[0b11] MOD=3 REG[rrr] RM[nnn]
OPERANDS  : REG0=YMM_R():rw:qq:f32 REG1=YMM_N():r:qq:f32 REG2=YMM_B():r:qq:f32

}
{
ICLASS    : VFNMADD231SD
EXCEPTIONS: avx-type-3
CPL       : 3
CATEGORY  : VFMA
EXTENSION : FMA
ATTRIBUTES: MXCSR simd_scalar
# R/M 128
PATTERN : VV1 0xBD V66 V0F38 W1 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()
OPERANDS  : REG0=XMM_R():rw:dq:f64 REG1=XMM_N():r:q:f64 MEM0:r:q:f64
# R/R 128
PATTERN : VV1 0xBD V66 V0F38 W1 MOD[0b11] MOD=3 REG[rrr] RM[nnn]
OPERANDS  : REG0=XMM_R():rw:dq:f64 REG1=XMM_N():r:q:f64 REG2=XMM_B():r:q:f64

}
{
ICLASS    : VFNMADD231SS
EXCEPTIONS: avx-type-3
CPL       : 3
CATEGORY  : VFMA
EXTENSION : FMA
ATTRIBUTES: MXCSR simd_scalar
# R/M 128
PATTERN : VV1 0xBD V66 V0F38 W0 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()
OPERANDS  : REG0=XMM_R():rw:dq:f32 REG1=XMM_N():r:d:f32 MEM0:r:d:f32
# R/R 128
PATTERN : VV1 0xBD V66 V0F38 W0 MOD[0b11] MOD=3 REG[rrr] RM[nnn]
OPERANDS  : REG0=XMM_R():rw:dq:f32 REG1=XMM_N():r:d:f32 REG2=XMM_B():r:d:f32

}

###################################################


{
ICLASS    : VFNMSUB132PD
EXCEPTIONS: avx-type-2
CPL       : 3
CATEGORY  : VFMA
EXTENSION : FMA
ATTRIBUTES: MXCSR
# R/M 128
PATTERN : VV1 0x9E VL128 V66 V0F38 W1 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()
OPERANDS  : REG0=XMM_R():rw:dq:f64 REG1=XMM_N():r:dq:f64 MEM0:r:dq:f64
# R/R 128
PATTERN : VV1 0x9E VL128 V66 V0F38 W1 MOD[0b11] MOD=3 REG[rrr] RM[nnn]
OPERANDS  : REG0=XMM_R():rw:dq:f64 REG1=XMM_N():r:dq:f64 REG2=XMM_B():r:dq:f64


# R/M 256
PATTERN : VV1 0x9E VL256 V66 V0F38 W1 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()
OPERANDS  : REG0=YMM_R():rw:qq:f64 REG1=YMM_N():r:qq:f64 MEM0:r:qq:f64
# R/R 256
PATTERN : VV1 0x9E VL256 V66 V0F38 W1 MOD[0b11] MOD=3 REG[rrr] RM[nnn]
OPERANDS  : REG0=YMM_R():rw:qq:f64 REG1=YMM_N():r:qq:f64 REG2=YMM_B():r:qq:f64
}
{
ICLASS    : VFNMSUB132PS
EXCEPTIONS: avx-type-2
CPL       : 3
CATEGORY  : VFMA
EXTENSION : FMA
ATTRIBUTES: MXCSR
# R/M 128
PATTERN : VV1 0x9E VL128 V66 V0F38 W0 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()
OPERANDS  : REG0=XMM_R():rw:dq:f32 REG1=XMM_N():r:dq:f32 MEM0:r:dq:f32
# R/R 128
PATTERN : VV1 0x9E VL128 V66 V0F38 W0 MOD[0b11] MOD=3 REG[rrr] RM[nnn]
OPERANDS  : REG0=XMM_R():rw:dq:f32 REG1=XMM_N():r:dq:f32 REG2=XMM_B():r:dq:f32


# R/M 256
PATTERN : VV1 0x9E VL256 V66 V0F38 W0 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()
OPERANDS  : REG0=YMM_R():rw:qq:f32 REG1=YMM_N():r:qq:f32 MEM0:r:qq:f32
# R/R 256
PATTERN : VV1 0x9E VL256 V66 V0F38 W0 MOD[0b11] MOD=3 REG[rrr] RM[nnn]
OPERANDS  : REG0=YMM_R():rw:qq:f32 REG1=YMM_N():r:qq:f32 REG2=YMM_B():r:qq:f32
}
{
ICLASS    : VFNMSUB132SD
EXCEPTIONS: avx-type-3
CPL       : 3
CATEGORY  : VFMA
EXTENSION : FMA
ATTRIBUTES: MXCSR simd_scalar
# R/M 128
PATTERN : VV1 0x9F V66 V0F38 W1 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()
OPERANDS  : REG0=XMM_R():rw:dq:f64 REG1=XMM_N():r:q:f64 MEM0:r:q:f64
# R/R 128
PATTERN : VV1 0x9F V66 V0F38 W1 MOD[0b11] MOD=3 REG[rrr] RM[nnn]
OPERANDS  : REG0=XMM_R():rw:dq:f64 REG1=XMM_N():r:q:f64 REG2=XMM_B():r:q:f64
}
{
ICLASS    : VFNMSUB132SS
EXCEPTIONS: avx-type-3
CPL       : 3
CATEGORY  : VFMA
EXTENSION : FMA
ATTRIBUTES: MXCSR simd_scalar
# R/M 128
PATTERN : VV1 0x9F  V66 V0F38 W0 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()
OPERANDS  : REG0=XMM_R():rw:dq:f32 REG1=XMM_N():r:d:f32 MEM0:r:d:f32
# R/R 128
PATTERN : VV1 0x9F  V66 V0F38 W0 MOD[0b11] MOD=3 REG[rrr] RM[nnn]
OPERANDS  : REG0=XMM_R():rw:dq:f32 REG1=XMM_N():r:d:f32 REG2=XMM_B():r:d:f32

}

{
ICLASS    : VFNMSUB213PD
EXCEPTIONS: avx-type-2
CPL       : 3
CATEGORY  : VFMA
EXTENSION : FMA
ATTRIBUTES: MXCSR
# R/M 128
PATTERN : VV1 0xAE VL128 V66 V0F38 W1 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()
OPERANDS  : REG0=XMM_R():rw:dq:f64 REG1=XMM_N():r:dq:f64    MEM0:r:dq:f64
# R/R 128
PATTERN : VV1 0xAE VL128 V66 V0F38 W1 MOD[0b11] MOD=3 REG[rrr] RM[nnn]
OPERANDS  : REG0=XMM_R():rw:dq:f64 REG1=XMM_N():r:dq:f64 REG2=XMM_B():r:dq:f64


# R/M 256
PATTERN : VV1 0xAE VL256 V66 V0F38 W1 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()
OPERANDS  : REG0=YMM_R():rw:qq:f64 REG1=YMM_N():r:qq:f64    MEM0:r:qq:f64
# R/R 256
PATTERN : VV1 0xAE VL256 V66 V0F38 W1 MOD[0b11] MOD=3 REG[rrr] RM[nnn]
OPERANDS  : REG0=YMM_R():rw:qq:f64 REG1=YMM_N():r:qq:f64 REG2=YMM_B():r:qq:f64
}
{
ICLASS    : VFNMSUB213PS
EXCEPTIONS: avx-type-2
CPL       : 3
CATEGORY  : VFMA
EXTENSION : FMA
ATTRIBUTES: MXCSR
# R/M 128
PATTERN : VV1 0xAE VL128 V66 V0F38 W0 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()
OPERANDS  : REG0=XMM_R():rw:dq:f32 REG1=XMM_N():r:dq:f32 MEM0:r:dq:f32
# R/R 128
PATTERN : VV1 0xAE VL128 V66 V0F38 W0 MOD[0b11] MOD=3 REG[rrr] RM[nnn]
OPERANDS  : REG0=XMM_R():rw:dq:f32 REG1=XMM_N():r:dq:f32 REG2=XMM_B():r:dq:f32


# R/M 256
PATTERN : VV1 0xAE VL256 V66 V0F38 W0 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()
OPERANDS  : REG0=YMM_R():rw:qq:f32 REG1=YMM_N():r:qq:f32 MEM0:r:qq:f32
# R/R 256
PATTERN : VV1 0xAE VL256 V66 V0F38 W0 MOD[0b11] MOD=3 REG[rrr] RM[nnn]
OPERANDS  : REG0=YMM_R():rw:qq:f32 REG1=YMM_N():r:qq:f32 REG2=YMM_B():r:qq:f32
}
{
ICLASS    : VFNMSUB213SD
EXCEPTIONS: avx-type-3
CPL       : 3
CATEGORY  : VFMA
EXTENSION : FMA
ATTRIBUTES: MXCSR simd_scalar
# R/M 128
PATTERN : VV1 0xAF  V66 V0F38 W1 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()
OPERANDS  : REG0=XMM_R():rw:dq:f64 REG1=XMM_N():r:q:f64     MEM0:r:q:f64
# R/R 128
PATTERN : VV1 0xAF  V66 V0F38 W1 MOD[0b11] MOD=3 REG[rrr] RM[nnn]
OPERANDS  : REG0=XMM_R():rw:dq:f64 REG1=XMM_N():r:q:f64 REG2=XMM_B():r:q:f64

}
{
ICLASS    : VFNMSUB213SS
EXCEPTIONS: avx-type-3
CPL       : 3
CATEGORY  : VFMA
EXTENSION : FMA
ATTRIBUTES: MXCSR simd_scalar
# R/M 128
PATTERN : VV1 0xAF  V66 V0F38 W0 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()
OPERANDS  : REG0=XMM_R():rw:dq:f32 REG1=XMM_N():r:d:f32     MEM0:r:d:f32
# R/R 128
PATTERN : VV1 0xAF  V66 V0F38 W0 MOD[0b11] MOD=3 REG[rrr] RM[nnn]
OPERANDS  : REG0=XMM_R():rw:dq:f32 REG1=XMM_N():r:d:f32 REG2=XMM_B():r:d:f32
}

{
ICLASS    : VFNMSUB231PD
EXCEPTIONS: avx-type-2
CPL       : 3
CATEGORY  : VFMA
EXTENSION : FMA
ATTRIBUTES: MXCSR
# R/M 128
PATTERN : VV1 0xBE VL128 V66 V0F38 W1 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()
OPERANDS  : REG0=XMM_R():rw:dq:f64 REG1=XMM_N():r:dq:f64 MEM0:r:dq:f64
# R/R 128
PATTERN : VV1 0xBE VL128 V66 V0F38 W1 MOD[0b11] MOD=3 REG[rrr] RM[nnn]
OPERANDS  : REG0=XMM_R():rw:dq:f64 REG1=XMM_N():r:dq:f64 REG2=XMM_B():r:dq:f64


# R/M 256
PATTERN : VV1 0xBE VL256 V66 V0F38 W1 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()
OPERANDS  : REG0=YMM_R():rw:qq:f64 REG1=YMM_N():r:qq:f64 MEM0:r:qq:f64
# R/R 256
PATTERN : VV1 0xBE VL256 V66 V0F38 W1 MOD[0b11] MOD=3 REG[rrr] RM[nnn]
OPERANDS  : REG0=YMM_R():rw:qq:f64 REG1=YMM_N():r:qq:f64 REG2=YMM_B():r:qq:f64

}
{
ICLASS    : VFNMSUB231PS
EXCEPTIONS: avx-type-2
CPL       : 3
CATEGORY  : VFMA
EXTENSION : FMA
ATTRIBUTES: MXCSR
# R/M 128
PATTERN : VV1 0xBE VL128 V66 V0F38 W0 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()
OPERANDS  : REG0=XMM_R():rw:dq:f32 REG1=XMM_N():r:dq:f32 MEM0:r:dq:f32
# R/R 128
PATTERN : VV1 0xBE VL128 V66 V0F38 W0 MOD[0b11] MOD=3 REG[rrr] RM[nnn]
OPERANDS  : REG0=XMM_R():rw:dq:f32 REG1=XMM_N():r:dq:f32 REG2=XMM_B():r:dq:f32

# R/M 256
PATTERN : VV1 0xBE VL256 V66 V0F38 W0 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()
OPERANDS  : REG0=YMM_R():rw:qq:f32 REG1=YMM_N():r:qq:f32 MEM0:r:qq:f32
# R/R 256
PATTERN : VV1 0xBE VL256 V66 V0F38 W0 MOD[0b11] MOD=3 REG[rrr] RM[nnn]
OPERANDS  : REG0=YMM_R():rw:qq:f32 REG1=YMM_N():r:qq:f32 REG2=YMM_B():r:qq:f32

}
{
ICLASS    : VFNMSUB231SD
EXCEPTIONS: avx-type-3
CPL       : 3
CATEGORY  : VFMA
EXTENSION : FMA
ATTRIBUTES: MXCSR simd_scalar
# R/M 128
PATTERN : VV1 0xBF V66 V0F38 W1 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()
OPERANDS  : REG0=XMM_R():rw:dq:f64 REG1=XMM_N():r:q:f64 MEM0:r:q:f64
# R/R 128
PATTERN : VV1 0xBF V66 V0F38 W1 MOD[0b11] MOD=3 REG[rrr] RM[nnn]
OPERANDS  : REG0=XMM_R():rw:dq:f64 REG1=XMM_N():r:q:f64 REG2=XMM_B():r:q:f64

}
{
ICLASS    : VFNMSUB231SS
EXCEPTIONS: avx-type-3
CPL       : 3
CATEGORY  : VFMA
EXTENSION : FMA
ATTRIBUTES: MXCSR simd_scalar
# R/M 128
PATTERN : VV1 0xBF V66 V0F38 W0 MOD[mm] MOD!=3 REG[rrr] RM[nnn] MODRM()
OPERANDS  : REG0=XMM_R():rw:dq:f32 REG1=XMM_N():r:d:f32 MEM0:r:d:f32
# R/R 128
PATTERN : VV1 0xBF V66 V0F38 W0 MOD[0b11] MOD=3 REG[rrr] RM[nnn]
OPERANDS  : REG0=XMM_R():rw:dq:f32 REG1=XMM_N():r:d:f32 REG2=XMM_B():r:d:f32

}

###################################################




